{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc5UaXX-7ewy"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 파라미터 저장할 폴더 생성\n",
        "if not os.path.exists('checkpoints'):\n",
        "  os.makedirs('checkpoints')\n"
      ],
      "metadata": {
        "id": "Mugh-WCf73Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 출력 저장할 폴더 생성\n",
        "if not os.path.exists('outputs'):\n",
        "  os.makedirs('outputs')"
      ],
      "metadata": {
        "id": "Ih5qb8vv8ATV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator networks - 구분자!!! 참, 거짓만 밝혀내면 되므로 출력은 1차원이면 됨\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, image_size=128):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.cls = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(256 * image_size // 16 * image_size // 16, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, img):\n",
        "    validity = self.cls(img)\n",
        "    return validity"
      ],
      "metadata": {
        "id": "ejnCRg4o8HJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator networks\n",
        "class Generator(nn.Module):\n",
        "\n",
        "  def __init__(self, image_size=128, latent_dim=512):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    self.decoder_input = nn.Linear(latent_dim, 256 * (image_size // 16) * (image_size //16))\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.image_size = image_size\n",
        "\n",
        "  def forward(self, z):\n",
        "    x = self.decoder_input(z)\n",
        "    x = x.view(-1, 256, (self.image_size // 16), (self.image_size // 16))\n",
        "    x = self.decoder(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "CGPrhRQD80bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show(img, renorm=False, nrow=8, interpolation='bicubic'):\n",
        "  if renorm:\n",
        "    img = img*0.5 + 0.5\n",
        "  img_grid = torchvision.utils.make_grid(img, nrow=nrow).numpy()\n",
        "  plt.figure()\n",
        "  plt.imshow(np.transpose(img_grid, (1,2,0)), interpolation=interpolation)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "VNLNucgU9rEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = 'data_faces/img_aling_celeba'\n",
        "img_list = os.listdir(root)\n",
        "print(len(img_list))"
      ],
      "metadata": {
        "id": "UWFSyGNU-Py6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_size = 128\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size=(re_size, re_size), interpolation=Image.BICUBIC),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "batch_size = 64\n",
        "celeba_data = datasets.ImageFolder('./data_faces', transform=transform)\n",
        "celeba_loader= DataLoader(celeba_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "sG6X98mT-WgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch, _ = next(iter(celeba_loader))\n",
        "show(batch[0:16], nrow=4)"
      ],
      "metadata": {
        "id": "Fs2PHkjh-xjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 생성자, 판별자 만들기\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# loss 함수 설정\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "UDRf00SV-3bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "HhQ3Q2GY8viC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VAE 훈련\n",
        "num_epochs = 50\n",
        "latent_dim = 512\n",
        "\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "# 훈련 루프\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (imgs, _) in enumerate(celeba_loader):\n",
        "\n",
        "    # Adversarial ground truths\n",
        "    valid = torch.ones(imgs.size[0], 1)\n",
        "    fake = torch.zeros(imgs.size[0], 1)\n",
        "\n",
        "    # generator 훈련\n",
        "    optimizer_G.zero_grad()\n",
        "    z = torch.randn(imgs.size[0], latent_dim)\n",
        "    gen_imgs = generator(z)\n",
        "    g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "    g_loss.backward()\n",
        "    optimizer_G.step()\n",
        "\n",
        "    # discriminator 훈련\n",
        "    optimizer_D.zero_grad()\n",
        "\n",
        "    real_loss = adversarial_loss(discriminator(imgs), valid)\n",
        "    fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "    d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "    d_loss.backward()\n",
        "    optimizer_D.step()\n",
        "\n",
        "    # loss 저장\n",
        "\n",
        "    G_losses.append(g_loss.item())\n",
        "    D_losses.append(d_loss.item())\n",
        "\n",
        "    # 진행상황 확인\n",
        "    if i % 100 == 0:\n",
        "      print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i} /  {len(celeba_loader)}] [D Loss: {d_loss.item()}][G Loss: {g_loss.item()}]\")\n",
        "\n",
        "      with torch.no_grad():\n",
        "        gen_imgs = generator(z)\n",
        "        # gen_img = 0.5 * gen_imgs + 0.5 # denormalize\n",
        "        gen_imgs = gen_imgs.permute(0, 2, 3, 1).clip(0,1).detach().numpy()\n",
        "        fig, axs = plt.subplots(5,5)\n",
        "        cnt = 0\n",
        "        for i in range(5):\n",
        "          for j in range(5):\n",
        "            axs[i, j].imshow(gen_imgs[cnt])\n",
        "            axs[i, j].axis('off')\n",
        "            cnt += 1\n",
        "        plt.savefig(os.path.join('outputs', f'gan_images_epoch_{epoch}.png'))\n",
        "        plt.close()\n",
        "\n",
        "torch.save(generator.state_dict(), os.path.join('checkpoints', f'G_{epoch}.png'))\n",
        "torch.save(discriminator.state_dict(), os.path.join('checkpoints', f'D_{epoch}.png'))\n",
        "# 나중에 보면 알겠지만, 이 GAN의 기본 구조도 문제가 있음..."
      ],
      "metadata": {
        "id": "iB2XGN-d800J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
