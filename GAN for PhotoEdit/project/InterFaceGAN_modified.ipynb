{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJnFlXf-QkDN"
      },
      "outputs": [],
      "source": [
        "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from mpl_toolkits.axes_gridl import ImageGrid\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import copy\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "tctVnjp7QxD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "5Z2T-YgIQzg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stylegan2-ada의 git을 복사하기\n",
        "# align_images.py를 추가하기\n",
        "if not os.path.isdir(\"stylegan2-ada-pytorch\"):\n",
        "  !git clone https://github.com/rkuo2000/stylegan2-ada-pytorch\n",
        "%cd stylegan2-ada-pytorch"
      ],
      "metadata": {
        "id": "wkeGt09HQ0Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('pretrained'):\n",
        "  !mkdir pretrained\n",
        "  %cd pretrained\n",
        "  !wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n",
        "  %cd .."
      ],
      "metadata": {
        "id": "IcmwMVhPQ3CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습된 모델 불러오기"
      ],
      "metadata": {
        "id": "2xX1qLRgRETo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pretrained/ffhq.pkl', 'rb') as f:\n",
        "  G = pickle.load(f)['G_ema'].cuda()"
      ],
      "metadata": {
        "id": "OQZv3T2IQ_I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "샘플 만들기"
      ],
      "metadata": {
        "id": "mkwyidIGRJPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id lzIEGC4uPVsjkZpL6be0UR2b4mQRq7rn9"
      ],
      "metadata": {
        "id": "-i-q4L_ARK_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device_name)"
      ],
      "metadata": {
        "id": "3dPj1iH0RNQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.from_numpy(np.load('projected_w.npy')).cuda().unsqueeze(0)\n",
        "img = G.synthesis(w, noise_mode='const')"
      ],
      "metadata": {
        "id": "iEY4zgx3RRnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow((img[0]*127.5 +128).permute(1,2,0).clamp(0, 255).detach().cpu().to(torch.uint8))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cJxlUyo2Rd8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id lwaSWjrS4StitDVtU2exsK9CWuk08kcX5\n",
        "!unzip SVM_vectors.zip"
      ],
      "metadata": {
        "id": "qR1URFnlR_ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attribute_number = 39"
      ],
      "metadata": {
        "id": "7DFBFOQ5SBZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_direction_Z = torch.load(f'SVM_vectors/class_direction_Z_on_3k_att{attribute_number}.pt').to(device)\n",
        "class_direction_W = torch.load(f'SVM_vectors/class_direction_W_on_3k_att{attribute_number}.pt').to(device)\n",
        "\n",
        "Z_intercept = torch.load(f'SVM_vectors/Z_intercept_on_3k_att{attribute_number}.pt').to(device)\n",
        "W_intercept = torch.load(f'SVM_vectors/W_intercept_on_3k_att{attribute_number}.pt').to(device)"
      ],
      "metadata": {
        "id": "5VqbGQqNSH9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IFGAN_AllLayersEdition(tensors_for_edition, direction, class_direction, b, G, n_steps_var = 7, scale_factor = 1., save=False):\n",
        "  \"\"\"\n",
        "  그림 편집을 latent space Z에서 InterFaceGAN 방식을 이용하여 편집하는 방법\n",
        "  class 1, class 2 vector를 지정하고, 수직한 벡터로 반대로 이동하게 하여 특성을 집어넣음\n",
        "  z: latent vectors\n",
        "  direction: - class -1방향\n",
        "  class_direction: data에서 hyperplane으로 수직으로 내린 벡터의 방향성\n",
        "  b: 하이퍼 평면의 특성 관련된 것 - hyperplane의 위치 등\n",
        "  G: Generator Network for StyleGAN2\n",
        "  scale_factor: 변화의 정도를 지정하는 변수\n",
        "\n",
        "  \"\"\"\n",
        "  rb_img = len(tensors_for_edition)\n",
        "\n",
        "  steps = torch.linspace(0.0, 1.0, n_steps_var)* scale_factor\n",
        "  fig = plt.figure(figsize=(2*n_steps_var, 2*nb_img))\n",
        "  grid = ImageGrid(fig, 111, nrows_n_cols=(nb_img, n_steps_var), axes_pad=0.1)\n",
        "\n",
        "  norm = torch.norm(class_direction)\n",
        "\n",
        "  for i_img, tensor in enumerate(tensors_for_edition):\n",
        "    for istep in range(n_steps_var):\n",
        "      tensor_transformed = tensor.clone()\n",
        "\n",
        "      distance = -(torch.dot(class_direction, tensor[0])+b)/norm**2 # tensor를 hyper plane으로 이동시키기 위함\n",
        "      tensor_transformed += class_direction*distance + steps[istep]*class_direction*direction\n",
        "      tensor_transformed = tensor_transformed.to(device)\n",
        "\n",
        "      img = G.synthesis(tensor_transformed[None, :, :]).cpu().detach().numpy()[0].transpose((1,2,0))\n",
        "      img = (img +1)/2\n",
        "      iplot = i_img * n_steps_var + istep\n",
        "      if istep == 0:\n",
        "        grid[iplot].set_ylabel(f'$\\mathrm{{IFG}}(\\mathbf{{v}}_{{ att = {attribute_number}}}, \\mathrm{{all}})$', rotation=0, size= 'x-large')\n",
        "        grid[iplot].yaxis.set_label_coords(-0.5, 0.45)\n",
        "\n",
        "      grid[iplot].imshow(np.clip((255*img).astype(int), 0, 255))\n",
        "      grid[iplot].set_yticklabels([])\n",
        "      grid[iplot].set_xticklabels([])\n",
        "      grid[iplot].set_xticks([])\n",
        "      grid[iplot].set_yticks([])\n",
        "  plt.title = (f\"{scale_factor=}\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  if save:\n",
        "    fig.savefig(\"all_components.png\")"
      ],
      "metadata": {
        "id": "y2er1CkASKZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IFGAN_AllLayersEdition(w, 1, class_direction_W, W_intercept, G, scale_factor=2.)"
      ],
      "metadata": {
        "id": "u-U4RZSASQkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_direction_W = class_direction_W.to(device)\n",
        "class_direction_Z = class_direction_Z.to(device)\n",
        "\n",
        "W_intercept = W_intercept.to(device)\n",
        "Z_intercept = Z_intercept.to(device)\n"
      ],
      "metadata": {
        "id": "_Y_fifTbSSgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scale_factor = 1.\n",
        "n_steps_var = 7\n",
        "direction = 1"
      ],
      "metadata": {
        "id": "4giGInoaSTC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ATTR_numbers = [31, 39, 15, 20, 22]\n",
        "ATTRS = ['Smiling', 'Young', 'Eyeglasses', 'Male', 'Mustache']\n",
        "directions = {}\n",
        "intercepts = {}\n",
        "for i, attr_name in enumerate(ATTRS):\n",
        "  directions[attr_name] = torch.load(f'SVM_vectors/direction_W_on_3k_att{ATTR_numbers[i]}.pt').to(device)\n",
        "  intercepts[attr_name] = torch.load(f'SVM_vectors/W_intercept_on_3k_att{ATTR_numbers[i]}.pt').to(device)\n"
      ],
      "metadata": {
        "id": "yfcPUtbYSVH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title {display-mode: \"form\", run: \"auto\"}\n",
        "\n",
        "Young =-2.4 #@param{type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "Eyeglasses = 0 #@param{type:\"slider\", min:-2.9, max:3.0, step:0.1}\n",
        "Male = 1.6 #@param{type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "Smiling = 0.9 #@param{type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "Mustache = 0 #@param{type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "\n",
        "direction=1\n",
        "tensor_transformed = w[0].clone()\n",
        "\n",
        "for i, attr_name in enumerate(ATTRS):\n",
        "  norm = torch.norm(directions[attr_name])\n",
        "\n",
        "  tensor_transformed += eval(attr_name)*directions[attr_name]*direction\n",
        "\n",
        "tensor_transformed = tensor_transformed.to(device)\n",
        "\n",
        "img = G.synthesis(tensor_transformed[None, :, :]).cpu().detach().numpy()[0].transpose((1,2,0))\n",
        "img = (img+1)/2\n",
        "plt.imshow(np.clip((255*img).astype(int), 0, 255))\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "WsTUIEPXSbzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval(attr_name)"
      ],
      "metadata": {
        "id": "xSYKF7WVSfxs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}