{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGmfP9HNEWo3"
      },
      "outputs": [],
      "source": [
        "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리 적용"
      ],
      "metadata": {
        "id": "Arc2i4diKfuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import copy\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "N4BN19yBEhAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 훈련된 모델 받기"
      ],
      "metadata": {
        "id": "DrG3BcpfKjS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.isdir('stylegan2-ada-pytorch')"
      ],
      "metadata": {
        "id": "QJiDsMqaJbI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('stylegan2-ada-pytorch'):\n",
        "  !git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "\n",
        "%cd stylegan2-ada-pytorch"
      ],
      "metadata": {
        "id": "87zM86twJfYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('pretrained'):\n",
        "  !mkdir pretrained\n",
        "  %cd pretrained\n",
        "  !wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n",
        "  %cd .."
      ],
      "metadata": {
        "id": "HaToQn8TJpyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pretrained/ffhq.pkl', 'rb') as f:\n",
        "  G = pickle.load(f)['G_ema'].cuda()\n"
      ],
      "metadata": {
        "id": "47DE_oUMKLjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 그림 생성하기"
      ],
      "metadata": {
        "id": "lKKd2nYEKmhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.randn((1, G.z_dim)).cuda()\n",
        "c = None\n",
        "\n",
        "img = G(z, c)"
      ],
      "metadata": {
        "id": "us1jayguJ8cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그림을 numpy의 배열로 변환"
      ],
      "metadata": {
        "id": "VfF5BIE4Kp1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_unit8 = (img[0]* 127.5 + 128).clamp(0,255).detach().permute(1,2,0).cpu().to(torch.uint8)"
      ],
      "metadata": {
        "id": "JHb5JFAyKTzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(target_uint8)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aVrQlPgnK7-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그림 정렬, 정돈\n",
        "### Double-cliquez pour modifier\n"
      ],
      "metadata": {
        "id": "qrqDpi78LM41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p raw\n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/0/0e/Donald_Trump_Pentagon_2017.jpg -0 raw/example.jpg"
      ],
      "metadata": {
        "id": "_p8aP500LVAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "그림에서 얼굴을 뽑아내고, 정렬하기 - DLib와 그 함수를 이용하여 원본 FFHQ 데이터셋의 준비과정을 거친 부분에서"
      ],
      "metadata": {
        "id": "hUoyICRmLhX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python align_images.py raw aligned"
      ],
      "metadata": {
        "id": "Nj0BKJxhLWYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "샘플로 만든 그림을 이미 훈련된 모델의 latent space로 변환"
      ],
      "metadata": {
        "id": "6WV-tV4DODYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "cJJ6X2-3LeId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_flickr_dog_path = os.path.join('aligned', 'example_01.png')\n",
        "target_img = Image.open(sample_flickr_dog_path)\n",
        "target_img"
      ],
      "metadata": {
        "id": "u5S3gE8iOKQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_uint8 = np.array(target_img, dtype=np.uint8)\n"
      ],
      "metadata": {
        "id": "KYdiCbDUOSuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미 훈련된 Generator 불러오기"
      ],
      "metadata": {
        "id": "Mg-g1dhJOh89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "G_eval = copy.deepcopy(G).eval().requires_grad_(False).to(device) # 모델을 평가 형태로 쓰기!!!"
      ],
      "metadata": {
        "id": "a3LpbqJIOg3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "w stats 계산"
      ],
      "metadata": {
        "id": "AAv1YBc8Ow5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z_samples = np.random.randn(10000, G_eval.z_dim)\n",
        "w_samples = G_eval.mapping(torch.from_numpy(z_samples).to(device), None)\n",
        "w_samples.size()"
      ],
      "metadata": {
        "id": "ZLHSuCeGOzEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)\n",
        "w_samples"
      ],
      "metadata": {
        "id": "EdPVMBJRPBqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_samples.shape"
      ],
      "metadata": {
        "id": "nQt5BaPlPImf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_avg = np.mean(w_samples, axis=0, keepdims=True)\n",
        "w_avg.shape"
      ],
      "metadata": {
        "id": "O5EVBINxPOlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_std = (np.sum((w_samples - w_avg) ** 2) / w_samples.shape[0]) ** 0.5\n",
        "w_std"
      ],
      "metadata": {
        "id": "UrF2TPiQPTex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "noise 입력 설정"
      ],
      "metadata": {
        "id": "nSZ5REDtPXj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_bufs = {name: buf for (name, buf) in G_eval.synthesis.named_buffers() if 'noise_const' in name}"
      ],
      "metadata": {
        "id": "BTj5FM07PVxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG16의 특성 감지 불러오기"
      ],
      "metadata": {
        "id": "zwUTaFixPj6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
        "with dnnlib.util.open_url(url) as f:\n",
        "  vgg16 = torch.jit.load(f).eval().to(device)"
      ],
      "metadata": {
        "id": "R4eHEnunPiG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "그림에서 특성 추출"
      ],
      "metadata": {
        "id": "Ch7w5ewxP1UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = torch.tensor(target_uint8, device=device).permute(2,0,1)\n",
        "target.size()"
      ],
      "metadata": {
        "id": "KDvPXqOTP0Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = target.unsqueeze(0).to(device).to(torch.float32)\n",
        "target.size()"
      ],
      "metadata": {
        "id": "RIgqQmZ-P9Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = F.interpolate(target, size=(256, 256), mode='area') # VGG16의 크기에 맞게 변경\n",
        "target.size()"
      ],
      "metadata": {
        "id": "YAP7LPzgQDlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_features = vgg16(target, resize_images=False, return_lpips=True)\n",
        "target_features"
      ],
      "metadata": {
        "id": "MFghq0cuQLTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_features.size()"
      ],
      "metadata": {
        "id": "nEsAAruUQQO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "최적화 설정,"
      ],
      "metadata": {
        "id": "u36RakOaQUJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 1000\n",
        "initial_learning_rate = 0.1\n",
        "\n",
        "w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True)\n",
        "w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)\n",
        "optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n",
        "\n",
        "# 노이즈 초기화\n",
        "for buf in noise_bufs.values():\n",
        "  buf[:] = torch.randn_like(buf)\n",
        "  buf.requires_grad = True"
      ],
      "metadata": {
        "id": "rdQ-ul5CSlfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "projection\n",
        "\n",
        "loss를 줄이기 위해 ....\n"
      ],
      "metadata": {
        "id": "XVBuoJhrS-A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 1000\n",
        "# 공식적으로 제공되는 기본 하이퍼 파라미터 값으로 지정\n",
        "lr_rampdown_length = 0.25\n",
        "lr_rampup_length = 0.05\n",
        "initial_noise_factor = 0.05\n",
        "noise_ramp_length = 0.75\n",
        "regularize_noise_weight = 1e5\n",
        "\n",
        "#\n",
        "best_loss = np.inf\n",
        "\n",
        "for step in tqdm(range(num_steps)):\n",
        "  # Learning rate를 적절하게 조절\n",
        "  t = step / num_steps\n",
        "  w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
        "  lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
        "  lr_ramp = 0.5 -0.5 * np.cos(lr_ramp * np.pi)\n",
        "  lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
        "  lr = initial_learning_rate * lr_ramp\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "  # 그림 생성\n",
        "  w_noise = torch.randn_like(w_opt) * w_noise_scale\n",
        "  ws = (w_opt + w_noise).repeat([1, G_eval.synthesis.num_ws, 1])\n",
        "  synth_images = G.synthesis(ws, noise_mode='const')\n",
        "\n",
        "  # 그림을 256x256 크기로 줄이기 VGG는 224x224의 그림에 맞게 설정되었음\n",
        "  synth_images = (synth_images + 1) * (255/2)\n",
        "  if synth_images.shape[2] > 256:\n",
        "    synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
        "\n",
        "  # 생성된 그림의 특성들\n",
        "  synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n",
        "  dist = (target_features - synth_features).square().sum() # 두 feature maps간의 차이를 계산 ( target, synth) > projection의 point\n",
        "\n",
        "  # noise 재정규화\n",
        "  reg_loss = 0.0\n",
        "  for v in noise_bufs.values():\n",
        "    noise = v[None,None,:,:] # F.avg_pool2d()에 쓰이려면 (1,1,H,W) 여야 함\n",
        "    while True:\n",
        "      reg_loss += (noise * torch.roll(noise, shifts=1, dims=3)).mean() ** 2\n",
        "      reg_loss += (noise * torch.roll(noise, shifts=1, dims=2)).mean() ** 2\n",
        "      if noise.shape[2] <= 8:\n",
        "        break\n",
        "      noise = F.avg_pool2d(noise, kernel_size=2)\n",
        "  loss = dist + reg_loss * regularize_noise_weight\n",
        "  # Step!!!\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f'step {step+1:>4d}/{num_steps}: dist {dist:<4.2f} loss {float(loss):<5.2f}')\n",
        "\n",
        "  # 각 optimization step마다 사영된 w를 저장\n",
        "  w_out[step] = w_opt.detach()[0]\n",
        "\n",
        "  # noise를 normalize\n",
        "  with torch.no_grad():\n",
        "    for buf in noise_bufs.values():\n",
        "      buf -= buf.mean()\n",
        "      buf *= buf.square().mean().rsqrt()\n"
      ],
      "metadata": {
        "id": "3CjYUN3hQS9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_out.size()"
      ],
      "metadata": {
        "id": "TdW1zBc3N1mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_out.repeat([1, G.mapping.num_w5, 1]).size()"
      ],
      "metadata": {
        "id": "Donb0h90Oazw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "projected_w_steps = w_out.repeat([1, G.mapping.num_ws, 1])\n",
        "projected_w = projected_w_steps[-1]\n",
        "print(projected_w.size())\n",
        "print(projected_w)"
      ],
      "metadata": {
        "id": "YYCEJrJWO-gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_w = projected_w.cpu().numpy()\n",
        "np.save('./projected_w', np_w)"
      ],
      "metadata": {
        "id": "j9rMFmnFPJUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synth_image = G.syntehsis(projected_w.unsqueeze(0), noise_mode='const')\n",
        "synth_image = (synth_image + 1) * (255/2)\n",
        "synth_image = synth_image.permute(0,2,3,1).clamp(0,255).to(torch.uint8)[0].cpu().numpy()"
      ],
      "metadata": {
        "id": "uWyyvxLZPO14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.concatenate([target_uint8, synth_image], axis=1))\n",
        "plt.axis('off')\n",
        "plt.title('synth <-- vs --> target')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7gwEAqgCPfQv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}