{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Style GAN 2를 pytorch로 구현!"
      ],
      "metadata": {
        "id": "Me7Z5zpQR1iW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuvSLoyDRx4C"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Tuple, Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "\n",
        "# Mapping Network를 구성하기!\n",
        "\n",
        "class MappingNetwork(nn.Module):\n",
        "  \"\"\"\n",
        "  Mapping Network\n",
        "\n",
        "  MLP with 8 linear layers\n",
        "  mapping network는 latent vector를 중간 단계의 latent space로 보내고,\n",
        "  그 space는 image space에서 특정 값들과 연계됨\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, features: int, n_layers: int):\n",
        "    \"\"\"\n",
        "    'features'는 특성의 갯수\n",
        "    'n_layers'는 mapping network의 layer의 갯수\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # MLP 생성\n",
        "    layers = []\n",
        "    for i in range(n_layers):\n",
        "      layers.append(EqualizedLinear(features, features))\n",
        "      layers.append(nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "    self.net = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, z: torch.Tensor):\n",
        "    # 표준화\n",
        "    z = F.normalize(z, dim=1)\n",
        "    #\n",
        "    return self.net(z)\n",
        "\n",
        "class EqualizedLinear(nn.Module):\n",
        "  \"\"\"\n",
        "  Learning-rate Equalized Linear Layer\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, in_features: int, out_features: int, bias: float = 0.):\n",
        "    super().__init__()\n",
        "    # Learning rate를 weight와 같게...\n",
        "    self.weight = EqualizedWeight([out_features, in_features])\n",
        "    # Bias\n",
        "    self.bias = nn.Parameter(torch.ones(out_features)*bias)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    # linear transform\n",
        "    return F.linear(x, self.weight(), bias=self.bias)\n",
        "\n",
        "\n",
        "class EqualizedWeight(nn.Module):\n",
        "  \"\"\"\n",
        "  optimizer는 learning rate에 따라 적용되나\n",
        "  effective weights는 learning rate에 따라 적용되지 않음\n",
        "  equalized learning rate없이는 효율적으로 학습이 되지 않음\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, shape: List[int]):\n",
        "    \"\"\"\n",
        "    'shape': weight parameter의 shape\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # 상수 초기화\n",
        "    self.c = 1/ math.sqrt(np.prod(shape[1:]))\n",
        "    # weight 초기화\n",
        "    self.weight = nn.Parameter(torch.randn(shape))\n",
        "\n",
        "  def forward(self):\n",
        "    # weight, constance 곱해서 돌려줌..\n",
        "    return self.weight * self.c\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class StyleBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Style Block\n",
        "  noise를 더하고, 그림의 품질, 그리고 스타일을 지정하는 block\n",
        "  weight modulation, convolution layer를 갖고 있음\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, d_latent: int, in_features: int, out_features: int):\n",
        "    \"\"\"\n",
        "    d_latent - dimensionality of weight\n",
        "    in_features - input feature map의 feature 숫자\n",
        "    out_features - output feature map의 feature 숫자\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "    # style vector를 weight에서 얻기\n",
        "    self.to_style = EqualizedLinear(d_latent, in_features, bias=1.0)\n",
        "    # wegiht modulated convolution layer\n",
        "    self.conv = Conv2dWeightModulate(in_features, out_features, kernel_size=3)\n",
        "    # noise scale\n",
        "    self.scale_noise = nn.Parameter(torch.zeros(1))\n",
        "    # Bias\n",
        "    self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "    # activation function\n",
        "    self.activation = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor, w: torch.Tensor, noise: Optional[torch.Tensor]):\n",
        "    \"\"\"\n",
        "    x 는 input feature map의 shape '[batch_size, in_features, height, width]'\n",
        "    w 는 weight와 shape '[batch_size, d_latent]'\n",
        "    noise는 tensor의 shape '[batch_size, 1, height, width]'\n",
        "    \"\"\"\n",
        "\n",
        "    # Style Vector 얻기\n",
        "    s = self.to_style(w)\n",
        "    # weight modulated convolution\n",
        "    x = self.conv(x, s)\n",
        "    # scale + noise\n",
        "    if noise is not None:\n",
        "      x = x + self.scale_noise[None, :, None, None] * noise\n",
        "    # bias더하고, activation function\n",
        "    return self.activation(x + self.bias[None, :, None, None])\n",
        "\n",
        "\n",
        "class Conv2WeightModulate(nn.Module):\n",
        "  \"\"\"\n",
        "  Convolution with weight modulation and Demodulation\n",
        "  해당 레이어는 convolution weights를 style vector와 scale을 하고, demodulates를 normalize로 나누어 진행\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, in_features: int, out_features: int, kernel_size:int, demodulate: bool=True, eps: float = 1e-8):\n",
        "    \"\"\"\n",
        "    in_feature는 input feature map의 feature 숫자\n",
        "    out_feature는 output feature map의 feature 숫자\n",
        "    kernel_size는 convolution kernel의 크기\n",
        "    demodulate는 normalize weight를 표준편차에 맞게 재조정할지 말지\n",
        "    eps는 normalizing에 활용되는 epsilon\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    #\n",
        "    self.out_features = out_features\n",
        "\n",
        "    self.demodulate = demodulate\n",
        "    self.padding = (kernal_size-1) // 2\n",
        "    # learning rate와 함께 조정되는 weight parameter\n",
        "    self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
        "\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x: torch.Tensor, s: torch.Tensor):\n",
        "    \"\"\"\n",
        "    x 는 input feature map의 shape '[batch_size, in_features, height, width]'\n",
        "    s 는 style기반의 scaling tensor의 shape '[batch_size, in_features]'\n",
        "\n",
        "    \"\"\"\n",
        "    # batch size, height, width\n",
        "    b, _, h, w = x.shape\n",
        "\n",
        "    # scale 형태 재조정\n",
        "    s = s[:, None, :, None, None]\n",
        "    # learning rate에 맞게 조정된 weights vector\n",
        "    weights = self.weight()[None, :, :, :, :]\n",
        "\n",
        "    # input channel, output channel, kernel index\n",
        "    # 결과로 얻는 shape은 '[batch_size, out_features, in_features, kernel_size, kernel_size]'\n",
        "    weights = weights * s\n",
        "\n",
        "    # demodulate\n",
        "    if self.demodulate:\n",
        "      # $$\\sigma_j = \\sqrt{\\sum_{i, k} {w1 _{i, j, k}}^2 + \\epsilon}$$\n",
        "      sigma_inv = torch.rsqrt((weights ** 2).sum(dim=(2,3,4), keepdim=True) + self.eps)\n",
        "\n",
        "      weights = weights * sigma_inv\n",
        "\n",
        "    # x reshape\n",
        "    x = x.reshape(1, -1, h, w)\n",
        "\n",
        "    # reshape weight\n",
        "    _, _, *ws = wieghts.shape\n",
        "    weights = weights.reshape(b * self.out_features, *ws)\n",
        "\n",
        "    # convolution을 계산할 때 그룹지어 계산하면 계산이 효율적임\n",
        "    # 그러나, batch에 있는 sample의 kenel weights가 다름\n",
        "    x = F.conv2d(x, weights, padding=self.padding, groups=b)\n",
        "\n",
        "    # x를 '[batch_size, out_features, height, width]'로 형태 재구성\n",
        "    return x.reshape(-1, self.out_features, h, w)\n"
      ],
      "metadata": {
        "id": "UMosUd0TXiuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "class GenerationBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  그림을 만드는 부분\n",
        "  $A$는 linear layer\n",
        "  $B$는 해상도를 올리고, 주변에 전파하는 역할을 담당 (noise는 단일 채널로 동작)\n",
        "  ['toRGB'] 는 간단한 style modulation을 갖고 있음\n",
        "  generator block은 2개의 style block과 하나의 RGB Block으로 구성됨\n",
        "  style block은 style modulation을 가진 3개의 convolution layer로 구성됨\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, d_latent: int, in_features: int, out_features: int):\n",
        "    \"\"\"\n",
        "    'd_latent'는 weights의 차원\n",
        "    'in_features'는 입력 특성맵의 갯수\n",
        "    'out_features'는 출력 특성맵의 갯수\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # 첫번째 style block은 feature map의 크기를 out_features의 크기로 만들어줌\n",
        "    self.style_block1 = StyleBlock(d_latent, in_features, out_features)\n",
        "    # 두 번째 style block\n",
        "    self.style_block2 = StyleBlock(d_latent, out_features, out_features)\n",
        "\n",
        "    # toRGB block\n",
        "    self.to_rgb = ToRGB(d_latent, out_features)\n",
        "\n",
        "\n",
        "  def forward(self, x:torch.Tensor, w:torch.Tensor, noise: Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]):\n",
        "    \"\"\"\n",
        "    x 는 input feature map의 shape '[batch_size, in_features, height, width]'\n",
        "    w 는 weight와 shape '[batch_size, d_latent]'\n",
        "    noise는 두 noise tensors의 shape '[batch_size, 1, height, width]'\n",
        "    \"\"\"\n",
        "\n",
        "    # 첫번째 style block은 첫 번째 noise tensor와 함께 동작\n",
        "    # 출력은 [batch_size, out_features, out_features, height, width]의 형태\n",
        "    x = self.style_block1(x, w, noise[0])\n",
        "    # 두 번째 style block은 두 번째 noise tensor와 함께 동작\n",
        "    # 출력은 [batch_size, out_features, out_features, height, width]의 형태\n",
        "    x = self.style_block2(x, w, noise[1])\n",
        "\n",
        "    # toRGB block\n",
        "    rgb = self.to_rgb(x, w)\n",
        "\n",
        "    # 특성 맵과 rgb 그림을 같이 돌려줌\n",
        "    return x, rgb"
      ],
      "metadata": {
        "id": "t0D_yDYUmLE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# toRGB\n",
        "class ToRGB(nn.Module):\n",
        "  \"\"\"\n",
        "  feature map을 이용하여 RGB 그림을 만듦, 1개의 convolution layer를 가짐\n",
        "  \"\"\"\n",
        "  def __init__(self, d_latent:int, features: int):\n",
        "    \"\"\"\n",
        "    d_latent: weights의 차원\n",
        "    features: feature map의 갯수\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "    # weight modulated convolution layer\n",
        "    self.to_style = EqualizedLinear(d_latent, features, bias=1.0)\n",
        "\n",
        "    # weight modulated convolution layer - demolation이 없는 layer\n",
        "    self.conv = Conv2dWeightModulate(features, 3, kernel_size=1, demodulate=False)\n",
        "\n",
        "    # bias\n",
        "    self.bias = nn.Parameter(torch.zeros(3))\n",
        "\n",
        "    # activation function\n",
        "    self.activation = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "  def forward(self, x:torch.Tensor, w:torch.Tensor):\n",
        "    \"\"\"\n",
        "    x 는 input feature map의 shape '[batch_size, in_features, height, width]'\n",
        "    w 는 weight와 shape '[batch_size, d_latent]'\n",
        "    \"\"\"\n",
        "\n",
        "    # style vector 얻기\n",
        "    style = self.to_style(w)\n",
        "    # weight modulated convolution\n",
        "    x = self.conv(x, style)\n",
        "    # bias와 evaluate activation function 적용\n",
        "    return self.activation(x + self.bias[None, :, None, None])\n",
        "\n"
      ],
      "metadata": {
        "id": "EkwPlzBdqZFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generator\n",
        "class Generator(nn.Module):\n",
        "  \"\"\"\n",
        "  Style GAN 2의 Generator\n",
        "\n",
        "  A는 linear layer로\n",
        "  B는 주변에 전파 및 규모를 키우는 역할을 수행(noise는 단일 채널)\n",
        "  toRGB는 style modulation을 갖고 있음\n",
        "\n",
        "  generator는 이미 학습한 상수로부터 시작\n",
        "  block들로 구성되어있는데, feature map의 해상도는 각 블록을 통과할 때마다 두 배로 증가\n",
        "  각 block은 rgb image를 출력하고, 크기가 증가되어, 최종 rgb 그림을 생성\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, log_resolution: int, d_latent: int, n_features: int=32, max_features: int=512):\n",
        "\n",
        "    # '[512, 512, 256, 128, 64, 32]\n",
        "    features = [min(max_features, n_features*(2** i)) for i in range(log_resolution-2, -1, -1)]\n",
        "    # generator block의 숫자\n",
        "    self.n_blocks = len(features)\n",
        "\n",
        "    # 훈련 가능한 4종류의 상수\n",
        "    self.initial_constant = nn.Parameter(torch.randn((1, features[0], 4, 4)))\n",
        "\n",
        "    # 첫 번째 style block - 4배로 키워줌\n",
        "    self.style_block = StyleBlock(d_latent, features[0], features[0])\n",
        "    self.to_rgb = ToRGB(d_latent, features[0])\n",
        "    # Generator block\n",
        "    blocks = [GeneratorBlock(d_latent, features[i-1], features[i]) for i in range(1, self.n_blocks)]\n",
        "    self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "    # 2배씩 증가하는 layer들 feature들은 각각 block 단위로 추출\n",
        "    self.up_sample = UpSample()\n",
        "\n",
        "\n",
        "  def forward(self, w: torch.Tensor, input_noise: List[Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]]):\n",
        "    \"\"\"\n",
        "    w 는 weight와 shape '[n_blocks, batch_size, d_latent]'\n",
        "    서로 다른 layer에서 사용하는 서로 다른 style을 섞기 위해 각 generator block마다 서로 구분된 w를 가짐\n",
        "\n",
        "    input_noise는 각 block마다 별개의 noise\n",
        "    noise sensors의 쌍으로 된 리스트는 각 block(최초의 block을 제외하고)마다 두 noise inputs를 갖고, convolution layer로 전달하기 때문\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # batch size 얻기\n",
        "    batch_size = w.shape[1]\n",
        "\n",
        "    # learned constant를 batch size에 맞게 확장\n",
        "    x = self.initial_constant.expand(batch_size, -1, -1, -1)\n",
        "\n",
        "    # 첫번째 style block\n",
        "    x = self.style_block(x, w[0], input_noise[0][1])\n",
        "\n",
        "    # 첫번째 rgb 그림\n",
        "    rgb = self.to_rgb(x, w[0])\n",
        "\n",
        "    # 나머지 block 평가\n",
        "    for i in range(1, self.n_blocks):\n",
        "      # 특성 맵을 추출하여 크기 키우기\n",
        "      x = self.up_sample(x)\n",
        "      # generator block을 통과\n",
        "      x, rgb_new = self.blocks[i-1](x, w[i], input_noise[i])\n",
        "      # rgb image를 추출하고, 크기 키우기, 그리고 block에서 얻은 rgb 더하기\n",
        "      rgb = self.up_sample(rgb) + rgb_new\n",
        "\n",
        "    # 최종 그림 결과 돌려주기\n",
        "    return rgb"
      ],
      "metadata": {
        "id": "tsHFpkVBu4TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator Block\n",
        "\n",
        "class DiscriminatorBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  discriminator block\n",
        "\n",
        "  2개의 $3 \\times 3$ convolution layer가 residual connection을 갖고 있음\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, in_features, out_features):\n",
        "    \"\"\"\n",
        "    in_features - input feature map의 feature 숫자\n",
        "    out_features - output feature map의 feature 숫자\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # 2개의 convolution\n",
        "\n",
        "    self.residual = nn.Sequential(DownSample(), EqualizedConv2d(in_features, out_features, kernel_size=1))\n",
        "\n",
        "    self.block = nn.Sequential(\n",
        "        EqualizedConv2d(in_features, out_features, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "        EqualizedConv2d(out_features, out_features, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "    )\n",
        "\n",
        "    self.down_sample = DownSample()\n",
        "\n",
        "    # 확장 파라미터\n",
        "    self.scale = 1/ math.sqrt(2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = self.residual(x)\n",
        "    # convolution\n",
        "    x = self.block(x)\n",
        "    # Downsample\n",
        "    x = self.down_sample(x)\n",
        "    return (x + residual) * self.scale\n"
      ],
      "metadata": {
        "id": "lFF95SC6zb1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# discriminator\n",
        "class Discriminator(nn.Module):\n",
        "  \"\"\"\n",
        "  Discriminator - image > feature map\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, log_resolution: int, n_features: int = 64, max_features: int = 512):\n",
        "    \"\"\"\n",
        "    'log_resolution' - log_2 image resolution\n",
        "    'n_features' - first block을 통해 뽑아내는 가장 높은 해상도의 feature\n",
        "    'max_features' - generator block에 있는 가장 높은 features\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "    # RGB 그림 > features map\n",
        "    self.from_rgb = nn.Sequential(\n",
        "        EqualizedConv2d(3, n_features, 1),\n",
        "        nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "    )\n",
        "    # 각 block에 들어갈 features 계산\n",
        "    features = [min(max_features, n_features * (2**i)) for i in range(log_resolution-1)]\n",
        "    # Discriminator의 block 갯수\n",
        "    n_blocks = len(featuers) - 1\n",
        "    # Discriminator block\n",
        "    blocks = [DiscriminatorBlock(features[i], features[i+1]) for i in range(n_blocks)]\n",
        "\n",
        "    self.blocks = nn.Sequential(*blocks)\n",
        "    # Mini-batch 표준편차\n",
        "    self.std_dev = MinibatchStdDev()\n",
        "    # 표준편차를 도입하고 난 뒤의 features 숫자\n",
        "    final_features = features[-1] +1\n",
        "    # Final convolution layer\n",
        "    self.conv = EqualizedConv2d(2*2*final_features, 1)\n",
        "    # Final linear layer - 분류를 위해 도입되는 최종\n",
        "    self.final = EqualizedLinear(2*2*final_features, 1)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    \"\"\"\n",
        "    'x' - 입력된 그림의 모습 [batch_size, 3, height, width]\n",
        "    \"\"\"\n",
        "\n",
        "    x = x - 0.5\n",
        "    # rgb그림을 입력 데이터 형태로 변경\n",
        "    x = self.from_rgb(x)\n",
        "    # discriminator block\n",
        "    x = self.blocks(x)\n",
        "    # mini-batch의 분산 계산, mini-batch표준편차 계산하고 붙이기\n",
        "    x = self.std_dev(x)\n",
        "    # convolution layer 통과\n",
        "    x = self.conv(x)\n",
        "    # 펼치기...\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    # 분류 확률 계산\n",
        "    return self.final(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "WJvVDYccVwWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path length regularization\n",
        "# 조금 더 random성을 부여하여... 그림에 랜덤성 등을 추가할 것\n",
        "\n",
        "class PathLengthPenalty(nn.Module):\n",
        "  \"\"\"\n",
        "  Path Length Penalty\n",
        "\n",
        "  이 재정규화 방식은 그림의 크기, 변화되는 값을 고정화 시켜 줌 > 그러니까.. 그림을 그려주거나 변주를 줄 때 지나친 연산을 막고, 그 변화의 방향성의 스텝 같은 것등을\n",
        "\n",
        "  $$\\mathbb{E}_{w \\sim f(z), y \\sim \\mathcal{N}(0, \\mathbf{I})}\n",
        "    \\Big(\\Vert \\mathbf{J}^\\top_{w} y \\Vert_2 - a \\Big)^2$$\n",
        "\n",
        "  $\\mathbf{J}_w$ 는 Jacobian\n",
        "  $\\mathbf{J}_w = \\frac{\\partial g}{\\partial w}$,\n",
        "  $w$는 \\w \\in \\mathcal{W}$에서 mapping network로 뽑은 것\n",
        "  $y$는 noise가 섞인 그림 $\\mathcal{N}(0, \\mathbf{I})$\n",
        "\n",
        "  $a$는 지수적으로 움직이는 평균 $\\Vert \\mathbf{J}^\\top_{w} y \\Vert_2$ 훈련 과정 중에 일어나는 움직임을 제어하는 파라미터 같음\n",
        "\n",
        "  $\\mathbf{J}^\\top_{w} y$는 Jacobian의 계산을 제외하고 계산한 것\n",
        "  $$\\mathbf{J}^\\top_{w} y = \\nabla_w \\big(g(w) \\cdot y \\big)$$\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, beat: float):\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, w: torch.Tensor, x: torch.Tensor):\n",
        "    \"\"\"\n",
        "    'w'는 batch의 모습 '[batch_size, d_latent]'\n",
        "    'x'는 생성되는 그림의 모습 '[batch_size, 3, height, width]'\n",
        "    \"\"\"\n",
        "    # 돌릴 device\n",
        "    device = x.device\n",
        "\n",
        "    # pixel갯수\n",
        "    image_size = x.shape[2] * x.shape[3]\n",
        "    # $y 계산 \\in \\mathcal{N}(0, \\mathbf{I})$\n",
        "    y = torch.randn(x.shape, device=device)\n",
        "\n",
        "    # 그림 크기 계산하기\n",
        "    # 해당 방식은 논문에서 언급되었으나,\n",
        "    output = (x*y).sum() / math.sqrt(image_size)\n",
        "\n",
        "    # gradient 계산\n",
        "    gradients, *_ = torch.autograd.grad(outputs=output,\n",
        "                                        inputs=w,\n",
        "                                        grad_outputs=torch.ones(output.shape, device=device),\n",
        "                                        create_graph=True)\n",
        "    # L2-norm 계산\n",
        "    norm = (gradient **2).sum(dim=2).mean(dim=1).sqrt()\n",
        "\n",
        "    # 첫번째 step 정규화\n",
        "    if self.steps>0:\n",
        "      # a 값 계산\n",
        "      a = self.exp_sum_a / (1-self.beta ** self.steps)\n",
        "      # 페널티 계산\n",
        "      loss = torch.mean((norm - a) ** 2)\n",
        "    else:\n",
        "      loss = norm.new_tensor(0)\n",
        "    # 평균 계산\n",
        "    mean = norm.mean().detach()\n",
        "    # exponental sum 더하기\n",
        "    self.exp_sum_a.mul_(self.beta).add_(mean, alpha=1 - self.beta)\n",
        "    #스텝 진행 상태 더하기\n",
        "    self.steps.add_(1.)\n",
        "    #페널티\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dmHZSJ4YeyMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련, 평가\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Integer, Tuple\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "\n",
        "from torchvision import datasets, transforms, utils\n",
        "\n",
        "from label_nn.gan.stylegan import Discriminator, Generator, MappingNetwork, GradientPenalty, PathLengthPenalty\n",
        "from label_nn.gan.wasserstein import DiscriminatorLoss, GeneratorLoss\n",
        "from label_nn.utils import cycle_dataloader\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "VoX8N-ERvbIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  \"\"\"\n",
        "  data set\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, path: str, image_size: int):\n",
        "    \"\"\"\n",
        "    'path'는 경로 지정\n",
        "    'image_size'는 그림 크기\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # '.jpg' 파일의 경로 확인\n",
        "    self.paths = [p for p in Path(path).glob(f'**/*.jpg')]\n",
        "\n",
        "    self.transform = torchvision.transforms.Compose([\n",
        "        # 그림 크기 변경\n",
        "        torchvision.transforms.Resize((image_size, image_size)),\n",
        "        # tensor로 변경\n",
        "        torchvision.transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"그림의 갯수\"\"\"\n",
        "    return len(self.paths)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"'index'번째의 그림\"\"\"\n",
        "    path = self.paths[index]\n",
        "    img = Image.open(path)\n",
        "    return self.transform(img)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "erbUeiXV5V6H",
        "outputId": "132354ac-2387-43fb-bfa4-9490c3d36e53"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-1-9527a0d3ce6e>, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-9527a0d3ce6e>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "u2-t9Mkw5fZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path: str = os.path.join('data_faces', 'img_align_celeba')\n",
        "\n",
        "# batch_size\n",
        "batch_size: int = 32\n",
        "# Dimension\n",
        "d_latent: int = 512\n",
        "\n",
        "image_size: int = 64\n",
        "# mapping network에 쓰일 layer의 갯수\n",
        "mapping_network_layers: int = 8"
      ],
      "metadata": {
        "id": "zdG2DGaJ8eyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Gradient Penalty Regularization Loss](index.html#gradient_penalty)\n",
        "gradient_penalty = GradientPenalty()\n",
        "# Gradient penalty coefficient\n",
        "gradient_penalty_coefficient: float = 10.0\n",
        "\n",
        "# [Path length penalty]()\n",
        "path_length_penalty: PathLengthPenalty"
      ],
      "metadata": {
        "id": "8UrpGT3N9Bg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 초기화\n",
        "\n",
        "dataset = Dataset(dataset_path, image_size)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "loader = cycle_dataloader(dataloader)"
      ],
      "metadata": {
        "id": "3L4bMSKs9dfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_resolution = int(math.log2(image_size))\n",
        "\n",
        "# discriminator, generator 객체 만들기\n",
        "discriminator = Discriminator(log_resolution).to(device)\n",
        "generator = Generator(log_resolution, d_latent).to(device)\n",
        "# Generator block에서 생성하는 꼴과 잡티 입력\n",
        "n_gen_blocks = generator.n_blocks\n",
        "# mapping_network 생성\n",
        "mapping_network = MappingNetwork(d_latent, mapping_network_layers).to(device)\n",
        "# path length penalty\n",
        "path_length_penalty = PathLengthPenalty(0.99).to(device)\n"
      ],
      "metadata": {
        "id": "rKF3VpcJ9vKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator, Discriminator의 learning rate\n",
        "learning_rate: float = 1e-3\n",
        "# mapping_network의 learning rate\n",
        "mapping_network_learning_rate: float = 1e-5\n",
        "# Number of steps to accumulate gradient\n",
        "# 이 숫자는 실질적으로 batch_size를 늘려주는 효과와 비슷함\n",
        "gradient_accumulate_steps: int = 1\n",
        "# Adam optimizer의 beta 파라미터 설정\n",
        "adam_betas: Tuple[float, float] = (0.0, 0.99)\n",
        "# 서로 다른 꼴 섞기\n",
        "style_mixing_prob: float = 0.9"
      ],
      "metadata": {
        "id": "3S8TAOtsArbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator, generator loss\n",
        "discriminator_loss = DiscriminatorLoss().to(device)\n",
        "generator_loss = GeneratorLoss().to(device)\n",
        "\n",
        "# optimizer\n",
        "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=adam_betas)\n",
        "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=adam_betas)\n",
        "mapping_network_optimizer = torch.optim.Adam(mapping_network.parameters(), lr=mapping_network_learning_rate, betas=adam_betas)\n"
      ],
      "metadata": {
        "id": "p17gp62vBW8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient penalty 계산하는 간격\n",
        "lazy_gradient_penalty_interval: int = 4\n",
        "# path length penalty 계산 간격\n",
        "lazy_path_length_penalty_interval: int = 32\n",
        "# 훈련 처음하는 시기엔 path length penalty 계산을 넘기기\n",
        "lazy_path_penalty_after: int 5_000\n",
        "\n",
        "# log로 생성된 그림들 보여주기\n",
        "log_generated_interval: int = 500\n",
        "# model을 얼마나 자주 저장할 것인가?\n",
        "save_checkpoint_interval: int = 2_000"
      ],
      "metadata": {
        "id": "upx6rebrBlgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"checkpoints\"):\n",
        "  os.makedirs(\"checkpoints\")"
      ],
      "metadata": {
        "id": "3nOqAio8CNkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_w(batch_size: int):\n",
        "  \"\"\"\n",
        "  표본\n",
        "  이 표본은 서로 다른 지점을 넘나들고, $w_1$이 서로 다른 지점을 넘기 전에 generator blocks에 적용되고, $w_2$가 generator blocks에 적용된 후에 넘어감\n",
        "  \"\"\"\n",
        "  # 꼴 섞기\n",
        "  if torch.rand(()).item()< style_mixing_prob:\n",
        "    # 무작위로 지점 넘나들기\n",
        "    cross_over_point = int(torch.rand(()).item() * n_gen_blocks)\n",
        "    # 표본 뽑기\n",
        "    z2 = torch.randn(batch_size, d_latent).to(device)\n",
        "    z1 = torch.randn(batch_size, d_latent).to(device)\n",
        "    # w1, w2 얻기\n",
        "    w1 = mapping_network(z1)\n",
        "    w2 = mapping_network(z2)\n",
        "    # w1, w2를 generator block에 적용하기 위해 ..\n",
        "    w1 = w1[None, :, :].expand(cross_over_point, -1, -1)\n",
        "    w2 = w2[None, :, :].expand(n_gen_blocks - cross_over_point, -1, -1)\n",
        "    return torch.cat((w1, w2), dim=0)\n",
        "  else:\n",
        "    z = torch.randn(batch_size, d_latent).to(device)\n",
        "    #\n",
        "    w = mapping_network(z)\n",
        "\n",
        "    return w[None, :, :].expand(n_gen_blocks, -1, -1)\n",
        "\n",
        "def get_noise(batch_size: int):\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "  # 잡티 저장할 곳\n",
        "  noise = []\n",
        "  #\n",
        "  resolution = 4\n",
        "  # generator noise\n",
        "  for i in range(n_gen_blocks):\n",
        "    # 첫번째 block은 convolution $3 \\times 3$개\n",
        "    if i == 0:\n",
        "      n1= None\n",
        "    # Generator 잡티는 첫 번째 layer 끝나고 더하기\n",
        "    else:\n",
        "      n1 = torch.randn(batch_size, 1, resolution, resolution, device=device)\n",
        "    # Generator 잡티를 두 번째 layer 끝나고 더하기\n",
        "    n2= torch.randn(batch_size, 1, resolution, resolution, device=device)\n",
        "    # noise tensor를 저장하기\n",
        "    noise.append((n1, n2))\n",
        "    # 다음 block의 해상도!!\n",
        "    resolution *= 2\n",
        "\n",
        "  return noise\n",
        "\n",
        "def generate_images(batch_size: int):\n",
        "  \"\"\"\n",
        "  그림 생성!!\n",
        "  \"\"\"\n",
        "  w = get_w(batch_size)\n",
        "  noise = get_noise(batch_size)\n",
        "\n",
        "  images = geneator(w, noise)\n",
        "  # 그림과 w 돌려주기\n",
        "  return images, w\n",
        "\n"
      ],
      "metadata": {
        "id": "va6hShFBCStR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step(idx: int):\n",
        "  \"\"\"\n",
        "  훈련 과정\n",
        "  \"\"\"\n",
        "  # reset gradient\n",
        "  discriminator_optimizer.zero_grad()\n",
        "\n",
        "  # 'gradient_accumulate_steps'로 gradient 계산 가속\n",
        "  for i in range(gradient_accumulate_steps):\n",
        "    # generator로 얻은 그림 표본\n",
        "    generated_images, _ = generate_images(batch_size)\n",
        "    # Discriminator로 그림 구분\n",
        "    fake_output = discriminator(generated_images.detach())\n",
        "\n",
        "    # 그림 폴더에서 가져오기\n",
        "    real_images = next(loader).to(device)\n",
        "    # gradient 계산하기\n",
        "    if (idx +1) % lazy_gradient_penalty_interval == 0:\n",
        "      real_images.requires_grad_()\n",
        "\n",
        "    # 실제 그림 discriminator로 그림 구분\n",
        "    real_output = discriminator(real_images)\n",
        "\n",
        "    # discriminator loss 계산\n",
        "    real_loss, fake_loss = discriminator_loss(real_output, fake_output)\n",
        "    disc_loss = real_loss + fake_loss\n",
        "    # gradient penalty 계산\n",
        "    if (idx +1) % lazy_gradient_penalty_interval == 0:\n",
        "      # gradient penalty 계산\n",
        "      gp = gradient_penalty(real_images, real_output)\n",
        "      # coeff 적용하고, gradient penalty 더하기\n",
        "      disc_loss = disc_loss + 0.5 * gradient_penalty_coefficient * gp * lazy_gradient_penalty_interval\n",
        "\n",
        "    # gradient 계산\n",
        "    disc_loss.backward()\n",
        "\n",
        "  # gradient 일부 자르기(안정성)\n",
        "  torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
        "  # optimizer step\n",
        "  discriminator_optimizer.step()\n",
        "\n",
        "  # reset gradient\n",
        "  generator_optimizer.zero_grad()\n",
        "  mapping_network_optimizer.zero_grad()\n",
        "\n",
        "  # 'gradient_accumulate_steps'로 gradient 계산 가속\n",
        "  for i in range(gradient_accumulate_steps):\n",
        "    # generator로 얻은 그림 표본\n",
        "    generated_images, w = generate_images(batch_size)\n",
        "    # Discriminator로 그림 구분\n",
        "    fake_output = discriminator(generated_images)\n",
        "\n",
        "    # generator loss 얻기\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    # path length penalty 더하기\n",
        "    if idx > lazy_path_penalty_after and (idx + 1) % lazy_path_penalty_interval == 0:\n",
        "      # path length penalty 계산\n",
        "      plp = path_length_penalty(w, generated_images)\n",
        "      # nan 예외\n",
        "      if not torch.isnan(plp):\n",
        "        gen_loss = gen_loss + plp\n",
        "\n",
        "    gen_loss.back_ward()\n",
        "\n",
        "    # gradient 계산\n",
        "    gen_loss.backward()\n",
        "  # 안정성을 위해 gradient 일부 자르기\n",
        "  torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
        "  torch.nn.utils.clip_grad_norm_(mapping_network.parameters(), max_norm=1.0)\n",
        "  # optimizer\n",
        "  generator_optimizer.step()\n",
        "  mapping_network_optimizer.step()\n",
        "\n",
        "  utils.save_image(\n",
        "      torch.cat([generated_images[:6], real_images[:3]], dim=0),\n",
        "      os.path.join('checkpoints', 'sample.png'),\n",
        "      nrow=3,\n",
        "      normalize=True,\n",
        "      value_range=(-1, 1)\n",
        "  )\n",
        "\n",
        "  # 모델 저장\n",
        "  if (idx + 1) % save_checkpoint_interval == 0:\n",
        "    torch.save(generator.state_dict(), os.path.join('checkpoints', 'generator.pth'))\n",
        "    torch.save(discriminator.state_dict(), os.path.join('checkpoints', 'discriminator.pth'))\n",
        "    torch.save(mapping_network.state_dict(), os.path.join('checkpoints', 'mapping_network.pth'))\n",
        ""
      ],
      "metadata": {
        "id": "W5QBoCzuGSBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 시킬 숫자\n",
        "training_steps: int = 150_000\n",
        "# 훈련 루프\n",
        "for i in tqdm(range(training_steps())):\n",
        "  step(i)\n",
        "\n"
      ],
      "metadata": {
        "id": "RuETAUJuOAp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Utay1CxNOLYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6nRiyx4cOurV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VIUJ7RBzO7aC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}