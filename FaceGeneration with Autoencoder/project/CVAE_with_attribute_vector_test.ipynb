{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXVd1n74xLgd"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CelebADataset(Dataset):\n",
        "  def __init__(self, data, labels, classes, transform=None):\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "    self.classes = classes\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img = Image.open(os.path.join('data_faces', 'img_align_celeba', self.data[idx])).convert('RGB')\n",
        "    label = torch.Tensor(self.labels[idx,[39,20,22,2,31,15]].astype('uint8'))\n",
        "\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    sample = {'images': img, 'labels': label}\n",
        "    return sample\n",
        "\n"
      ],
      "metadata": {
        "id": "uwWG8mfOxR_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.utils as vutils\n",
        "\n",
        "def display_grid(images, nrow=8, figsize=(12, 12)):\n",
        "  fig=plt.figure(figsize=figsize)\n",
        "  plt.imshow(np.transpose(vutils.make_grid(images, nrow=nrow, padding=2, normalize=True).cpu(), (1,2,0)))\n",
        "  plt.axis('off')\n"
      ],
      "metadata": {
        "id": "qNi8lc6CxX1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('list_attr_celeba.csv')\n"
      ],
      "metadata": {
        "id": "zUqOD13qxzW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size=(128, 128), interpolation=Image.BICUBIC),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "VAety030x5NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, labels = df.values[:, 0], (df.values[:, 1:]+1) //2\n",
        "classes = df.columns[1:]"
      ],
      "metadata": {
        "id": "OaBH0o7myFJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "celeba_data = CelebADataset(data, labels, classes, transform)\n",
        "celeba_loader = DataLoader(celeba_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "FZ442yfxyOYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "  def __init__(self, image_size=128, latent_dim=512):\n",
        "    super(VAE, self).__init__()\n",
        "\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten()\n",
        "    )\n",
        "    # embedding 차원 정의 -\n",
        "    self.Embeddings = nn.Embedding(2, 10) # 특성 표시 방법이 2가지 - 0, 1, 그리고 그림, 그리고 특성을 10개의 벡터 차원으로 표현\n",
        "    # 평균, 분산과 관련된 파라미터 정의\n",
        "    self.fc_mu = nn.Linear(256 * (image_size // 16) * (image_size // 16), latent_dim)\n",
        "    self.fc_var = nn.Linear(256 * (image_size // 16) * (image_size // 16), latent_dim)\n",
        "\n",
        "    self.decoder_input = nn.Linear(latent_dim+6*10, 256 * (image_size // 16) * (image_size // 16))\n",
        "    # 위에서 decoder의 입력으로 들어가는 차원에 추가되는 이유는 우리가 변경할 특성의 숫자가 6개고, 또한 그 특성은 각각 embeddings에서 10의 차원으로 정의하였기 때문에, 각 특성의 갯수 * embedding의 차원을 고려하여 추가되는 것\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.image_size = image_size\n",
        "\n",
        "  def encode(self, x):\n",
        "    x = self.encoder(x)\n",
        "    mu, logvar = self.fc_mu(x), self.fc_var(x)\n",
        "    return mu, logvar\n",
        "\n",
        "  def reparameterize(self, mu, logvar):\n",
        "    std = torch.exp(0.5*logvar)\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + eps*std\n",
        "\n",
        "  def decode(self, z):\n",
        "    x = self.decoder_input(z)\n",
        "    x = x.view(-1, 256, (self.image_size // 16), (self.image_size // 16))\n",
        "    x = self.decoder(x)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x, labels):\n",
        "    mu, logvar = self.encode(x)\n",
        "    z = self.reparameterize(mu, logvar)\n",
        "    B, _ = z.shape\n",
        "    z_labels = self.Embeddings(labels).reshape(B, -1)\n",
        "    z = torch.cat((z, z_labels), 1)\n",
        "    recon_x = self.decode(z)\n",
        "    return recon_x, mu, logvar\n"
      ],
      "metadata": {
        "id": "oOkOrBEqyV8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = VAE().to(device)\n",
        "model.load_state_dict(torch.load('vae_celeba.pt'))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ed80C8xQydDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = next(iter(celeba_loader))\n",
        "imgs, labels = data['images'].float().to(device), data['labels'].long().to(device)"
      ],
      "metadata": {
        "id": "9Ncf6JAVynsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  reconstructions, _, _ = model(imgs, labels)\n",
        "  reconstructions = reconstructions.detach().cpu().clip(0,1)\n",
        "\n",
        "display_grid(imgs.detach().cpu(), figsize=(8,8))\n",
        "plt.savefig('vae_training_data.png', bbox_inches='tight', pad_inches=0, dpi=156)\n",
        "plt.show()\n",
        "display_grid(reconstructions, figsize=(8,8))\n",
        "plt.savefig('vae_reconstruction.png', bbox_inches='tight', pad_inches=0, dpi=156)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_D3FvIxZyvbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_names = classes[[39, 20, 22, 2, 31, 15]]\n",
        "att_names"
      ],
      "metadata": {
        "id": "PnGqBVGzzXDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = np.random.choice(len(imgs))\n",
        "att_idx = 1\n",
        "\n",
        "with torch.no_grad():\n",
        "  reconstructions, _, _ = model(imgs[[sample_idx]], labels[[sample_idx]])\n",
        "  reconstructions = reconstructions.detach().cpu().clip(0,1)\n",
        "\n",
        "new_labels = labels[[sample_idx]]\n",
        "new_labels[:, att_idx]= 1-new_labels[:, att_idx]\n",
        "\n",
        "with torch.no_grad():\n",
        "  reconstructions2, _, _ = model(imgs[[sample_idx]], new_labels)\n",
        "  reconstructions2 = reconstructions2.detach().cpu().clip(0,1)\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.imshow(imgs[[sample_idx]].permute(0, 2, 3, 1).cpu().numpy()[0])\n",
        "plt.axis('off')\n",
        "plt.title('Original')\n",
        "plt.subplot(132)\n",
        "plt.imshow(reconstructions.permute(0, 2, 3, 1).cpu().numpy()[0])\n",
        "plt.axis('off')\n",
        "plt.title(f'{att_names[att_idx]} %d {%(labels[sample_idx, att_idx])}')\n",
        "plt.subplot(133)\n",
        "plt.imshow(reconstructions2.permute(0, 2, 3, 1).cpu().numpy()[0])\n",
        "plt.axis('off')\n",
        "plt.title(f'{att_names[att_idx]} %d {%(new_labels[sample_idx, att_idx])}')\n"
      ],
      "metadata": {
        "id": "UgUtH_V7zaYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzYyyJQ900p4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}