{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0HqUcYpmLCN"
      },
      "outputs": [],
      "source": [
        "#역시 CelebA 데이터셋을 이용하여\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir data_faces && wget https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip"
      ],
      "metadata": {
        "id": "XD3Oeg-lmSBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1Ewn2uWDzY9k9_00gCsoTsJk9AJveRLoJ"
      ],
      "metadata": {
        "id": "zhbTPI-qmcFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "with zipfile.Zipfile('list_attr_celeba.csv.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('./')\n",
        "# 압축 풀어서 하위 폴더에 넣기!"
      ],
      "metadata": {
        "id": "YDIJzH8pmlFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "hx-gfWnlmzrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show(img, renorm=False, nrow=8, interpolation='bicubic'):\n",
        "  if renorm:\n",
        "    img = img*0.5 + 0.5\n",
        "  img_grid = torchvision.utils.make_grid(img, nrow=nrow).numpy()\n",
        "  plt.figure()\n",
        "  plt.imshow(np.transpose(img_grid, (1,2,0)), interpolation=interpolation) # transpose는 그림의 데이터 차원 순서를 바꾸기, RGB의 순서가 torch와 plt의 처리 순서가 다르기 때문...\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "oxq5P3IInR-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#각 그림에 담긴 attribute-특성을 붙여야 함\n",
        "df = pd.read_csv('list_attr_celeba.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wGWCgAFpnt6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size=(128, 128), interpolation=Image.BICUBIC),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "SKmhQGdhn5_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, labels = df.values[:, 0], (df.values[:, 1:]+1)//2 #label은 -1과 1로 되어있어 해당 내용을 0, 1로 바꾸기\n",
        "\n",
        "classes = df.columns[1:]\n",
        "classes[[39,20,22,2,31,15]] # 그림으로 변경할 특성들 여섯개만 뽑기"
      ],
      "metadata": {
        "id": "1VB274IioGKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CelebADataset(Dataset):\n",
        "  def __init__(self, data, labels, classes, transform=None):\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "    self.classes = classes\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img = Image.open(os.path.join('data_faces', 'img_align_celeba', self.data[idx])).convert('RGB')\n",
        "    label = torch.Tensor(self.labels[idx,[39,20,22,2,31,15]].astype('uint8'))\n",
        "\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    sample = {'images': img, 'labels': label}\n",
        "    return sample\n",
        "\n"
      ],
      "metadata": {
        "id": "2JqHGsQOoYeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "celeba_data = CelebADataset(data, labels, classes, transform)"
      ],
      "metadata": {
        "id": "Y2LbISADp-zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "celeba_data[0]"
      ],
      "metadata": {
        "id": "UcuAzv1MqKFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "celeba_loader = DataLoader(celeba_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "r6QM7qAGqMcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch= next(iter(celeba_loader))\n",
        "\n",
        "show(batch['images'][0:16], renorm=True, nrow=4)"
      ],
      "metadata": {
        "id": "Adqbt0nYqhXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "  def __init__(self, image_size=128, latent_dim=512):\n",
        "    super(VAE, self).__init__()\n",
        "\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten()\n",
        "    )\n",
        "    # embedding 차원 정의 -\n",
        "    self.Embeddings = nn.Embedding(2, 10) # 특성 표시 방법이 2가지 - 0, 1, 그리고 그림, 그리고 특성을 10개의 벡터 차원으로 표현\n",
        "    # 평균, 분산과 관련된 파라미터 정의\n",
        "    self.fc_mu = nn.Linear(256 * (image_size // 16) * (image_size // 16), latent_dim)\n",
        "    self.fc_var = nn.Linear(256 * (image_size // 16) * (image_size // 16), latent_dim)\n",
        "\n",
        "    self.decoder_input = nn.Linear(latent_dim+6*10, 256 * (image_size // 16) * (image_size // 16))\n",
        "    # 위에서 decoder의 입력으로 들어가는 차원에 추가되는 이유는 우리가 변경할 특성의 숫자가 6개고, 또한 그 특성은 각각 embeddings에서 10의 차원으로 정의하였기 때문에, 각 특성의 갯수 * embedding의 차원을 고려하여 추가되는 것\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.image_size = image_size\n",
        "\n",
        "  def encode(self, x):\n",
        "    x = self.encoder(x)\n",
        "    mu, logvar = self.fc_mu(x), self.fc_var(x)\n",
        "    return mu, logvar\n",
        "\n",
        "  def reparameterize(self, mu, logvar):\n",
        "    std = torch.exp(0.5*logvar)\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + eps*std\n",
        "\n",
        "  def decode(self, z):\n",
        "    x = self.decoder_input(z)\n",
        "    x = x.view(-1, 256, (self.image_size // 16), (self.image_size // 16))\n",
        "    x = self.decoder(x)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x, labels):\n",
        "    mu, logvar = self.encode(x)\n",
        "    z = self.reparameterize(mu, logvar)\n",
        "    B, _ = z.shape\n",
        "    z_labels = self.Embeddings(labels).reshape(B, -1)\n",
        "    z = torch.cat((z, z_labels), 1)\n",
        "    recon_x = self.decode(z)\n",
        "    return recon_x, mu, logvar\n",
        "\n"
      ],
      "metadata": {
        "id": "93tEZC9Lqpsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "  BCE = nn.BCELoss(reduction='sum')(recon_x, x)\n",
        "  KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "  return BCE + KLD\n"
      ],
      "metadata": {
        "id": "PmVIombVuSyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "YxUqxZREubdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VAE().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "cmwdsLDrufZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "\n",
        "best_loss = np.inf\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0\n",
        "  cpt = 0\n",
        "  for batch_idx, data in enumerate(celeba_loader):\n",
        "    imgs, labels = data['images'].float().to(device), data['labels'].long().to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    recon_imgs, mu, logvar = model(imgs, labels)\n",
        "    loss = vae_loss(recon_imgs, imgs, mu, logvar)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "    cpt += 1\n",
        "\n",
        "    if batch_idx % 100 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(celeba_loader)}], Loss: {total_loss/cpt:4f}')\n",
        "  print(f'Epoch [{epoch+1}/{num_epochs}], Total Loss: {total_loss/len(celeba_loader):.4f}')\n",
        "  if total_loss < best_loss:\n",
        "    best_loss = total_loss\n",
        "    torch.save(model.state_dict(), 'celeba_vae.pth')"
      ],
      "metadata": {
        "id": "xibFozC-ulWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위에서의 모델의 훈련이 끝나면...?\n",
        "\n"
      ],
      "metadata": {
        "id": "eEsD49RVv2OW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}